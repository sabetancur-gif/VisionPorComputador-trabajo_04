<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Taller 1: Fundamentos y Calibración de Cámara</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
  <script src="https://unpkg.com/lucide@latest"></script>
  <style>
    :root {
      --accent: #0ea5a4;
      --bg: #f8f9fb;
      --text: #0f172a;
      --muted: #6b7280;
      --card: #ffffff;
      --shadow: 0 2px 8px rgba(0, 0, 0, 0.06);
    }

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: "Inter", sans-serif;
      background: var(--bg);
      color: var(--text);
      line-height: 1.6;
      display: flex;
    }

    /* SIDEBAR */
    .sidebar {
      position: fixed;
      top: 0;
      left: 0;
      width: 260px;
      height: 100%;
      background: var(--card);
      box-shadow: var(--shadow);
      padding: 1.5rem;
      transition: transform 0.3s ease-in-out;
      z-index: 200;
    }

    .sidebar.collapsed {
      transform: translateX(-100%);
    }

    .sidebar h2 {
      font-weight: 700;
      font-size: 1.1rem;
      margin-bottom: 1rem;
      color: var(--accent);
      text-transform: uppercase;
      letter-spacing: 0.05em;
    }

    .sidebar nav a {
      display: flex;
      align-items: center;
      gap: 0.6rem;
      text-decoration: none;
      color: var(--text);
      padding: 0.5rem 0.8rem;
      border-radius: 8px;
      transition: all 0.2s ease;
      font-weight: 500;
    }

    .sidebar nav a:hover {
      background: var(--accent);
      color: white;
      transform: translateX(4px);
    }

    /* HAMBURGER BUTTON */
    .hamburger {
      position: fixed;
      top: 1rem;
      left: 1rem;
      background: var(--card);
      border: none;
      box-shadow: var(--shadow);
      border-radius: 10px;
      padding: 0.6rem;
      cursor: pointer;
      z-index: 300;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: background 0.3s;
    }

    .hamburger:hover {
      background: var(--accent);
      color: white;
    }

    /* MAIN CONTENT */
    main {
      margin-left: 260px;
      padding: 2rem 3rem;
      max-width: 900px;
      width: 100%;
      transition: margin-left 0.3s ease-in-out;
    }

    main.collapsed {
      margin-left: 0;
    }

    h1 {
      font-size: 1.9rem;
      font-weight: 700;
      margin-bottom: 0.5rem;
      color: var(--accent);
    }

    h2 {
      font-size: 1.4rem;
      font-weight: 600;
      margin-top: 2.5rem;
      margin-bottom: 1rem;
      color: var(--text);
      border-bottom: 2px solid var(--accent);
      display: inline-block;
      padding-bottom: 0.3rem;
    }

    h3 {
      font-size: 1.1rem;
      margin-top: 1.4rem;
      margin-bottom: 0.5rem;
      font-weight: 600;
      color: var(--text);
    }

    p {
      margin-bottom: 1rem;
      text-align: justify;
    }

    ul {
      margin-left: 1.5rem;
      margin-bottom: 1rem;
    }

    li {
      margin-bottom: 0.5rem;
    }

    strong {
      color: var(--text);
    }

    @media (max-width: 900px) {
      .sidebar {
        transform: translateX(-100%);
      }

      .sidebar.active {
        transform: translateX(0);
      }

      main {
        margin-left: 0;
        padding: 1.5rem;
      }
    }

    /* FIGURES (IMAGES & VIDEO) */
    .figure {
      margin: 2rem auto;
      text-align: center;
    }

    .figure img {
      max-width: 650px;
      width: 100%;
      height: auto;
      display: block;
      margin: 0 auto;
    }

    .figure video {
      max-width: 720px;
      width: 100%;
      height: auto;
      display: block;
      margin: 0 auto;
      border-radius: 8px;
      box-shadow: var(--shadow);
    }

    .figure .caption {
      margin-top: 0.5rem;
      font-size: 0.9rem;
      color: var(--muted);
      text-align: center;
    }

  </style>
</head>
<body>
  <!-- Botón hamburguesa -->
  <button class="hamburger" id="menuBtn" aria-label="Abrir menú">
    <i data-lucide="menu"></i>
  </button>

  <!-- Sidebar -->
  <aside class="sidebar" id="sidebar">
    <h2>Contenido</h2>
    <nav>
      <a href="#introduccion"><i data-lucide="book-open"></i> Introducción</a>
      <a href="#explicacion"><i data-lucide="book-open"></i> Explicación</a>
      <a href="#marco"><i data-lucide="layers"></i> Marco teórico</a>
      <a href="#metodologia"><i data-lucide="flask-round"></i> Metodología</a>
      <a href="#resultados"><i data-lucide="bar-chart-3"></i> Resultados</a>
      <a href="#conclusion"><i data-lucide="check-circle"></i> Conclusión</a>
      <a href="#reporte"><i data-lucide="users"></i> Reporte individual</a>
      <a href="#referencias"><i data-lucide="book"></i> Referencias</a>
    </nav>
  </aside>

  <!-- Contenido -->
  <main id="main">
    <h1>Taller 4: Sistema de Detección y Seguimiento de Objetos en Video</h1>
    <p><strong>Visión por Computador</strong><br>
    <strong>Universidad Nacional de Colombia</strong><br>
    <strong>Docente:</strong> Juan David Ospina Arango<br>
    <strong>Estudiantes:</strong></p>
    <ul>
      <li>Santiago Betancur Montoya</li>
      <li>Reinaldo David Lopez Narvaez</li>
      <li>Jose Sebastian Garzon Parra</li>
      <li>Monica Paola Vargas Tirado</li>
    </ul>

    <h2 id="introduccion">Introducción</h2>

    <p>
    El análisis automatizado de secuencias de video es uno de los campos más relevantes dentro de la visión por computador, con aplicaciones directas en vigilancia urbana,
    transporte inteligente, conteo de personas, análisis de multitudes y sistemas de seguridad. En este contexto, la detección y el seguimiento de objetos en movimiento
    representan un problema central, ya que requieren no solo identificar objetos de interés en cada fotograma, sino también mantener su identidad a lo largo del tiempo.
    </p>

    <p>
    En este proyecto se desarrolla un sistema completo de detección y seguimiento de peatones, integrando un modelo moderno de detección basado en YOLOv8 con un método
    de seguimiento multiobjeto apoyado en técnicas de flujo óptico y emparejamiento por intersección sobre unión (IoU). Ambos componentes se articulan dentro de un
    pipeline unificado que permite procesar secuencias de imágenes de manera ordenada, reproducible y evaluable.
    </p>

    <p>
    El trabajo se basa en el dataset público MOT17, ampliamente utilizado en investigación académica para el problema de <i>Multiple Object Tracking (MOT)</i>.
    Este dataset contiene secuencias de video reales en entornos urbanos, con múltiples peatones, oclusiones parciales y variaciones en iluminación, lo que representa
    un escenario desafiante y realista para la evaluación de sistemas de seguimiento.
    </p>

    <p>
    El sistema desarrollado no solo realiza detección de peatones cuadro a cuadro, sino que además asigna identificadores persistentes a cada individuo, permitiendo
    analizar trayectorias, conteo de objetos y comportamiento temporal. El desempeño del sistema se evalúa mediante métricas cuantitativas de detección
    (precisión, recall y F1-score), así como mediante visualizaciones y videos anotados que facilitan el análisis cualitativo de los resultados obtenidos.
    </p>


    <h2 id="explicacion">Explicación del proyecto</h2> 
    <figure style="margin:1.25rem 0; max-width:900px;">
      <video controls playsinline preload="metadata" style="width:100%; height:auto; border-radius:8px; box-shadow:0 2px 8px rgba(0,0,0,0.08);">
        <source src="data/explicacion.mp4" type="video/mp4">
        Tu navegador no soporta la etiqueta video.
      </video>
      <figcaption style="font-size:0.9rem; color:var(--muted); margin-top:0.4rem;">
        Video 1 — Explicación de ejecución pipeline.
      </figcaption>
    </figure>

    <h2 id="marco">Marco teórico</h2>

    <ul>

      <li><strong>Detección de objetos con YOLO</strong></li>
      <p>
      YOLO (You Only Look Once) es una familia de modelos de detección de objetos basada en redes neuronales convolucionales que formulan la detección como un problema de
      regresión directa, permitiendo predecir simultáneamente las cajas delimitadoras, las clases y las probabilidades asociadas en una sola pasada por la imagen
      <a href="#lizardi2025">(Lizardi Vargas, 2025)</a>. Gracias a su alta eficiencia computacional y buen balance entre velocidad y precisión, YOLO se ha consolidado como
      una de las arquitecturas más utilizadas en aplicaciones de visión por computador en tiempo real.
      </p>

      <p>
      En este proyecto se emplea YOLOv8, una versión moderna que adopta una arquitectura <i>anchor-free</i>, un backbone optimizado y un diseño modular que facilita su
      integración mediante la librería Ultralytics. Para cada fotograma procesado, el detector genera un conjunto de cajas delimitadoras, etiquetas de clase y puntajes de
      confianza, los cuales sirven como entrada principal para el módulo de seguimiento.
      </p>

      <li><strong>Seguimiento de objetos mediante flujo óptico</strong></li>
      <p>
      El flujo óptico describe el desplazamiento aparente de los píxeles entre fotogramas consecutivos y permite estimar el movimiento local dentro de una secuencia de video
      <a href="#monzon2013">(Monzón López, 2013)</a>. Esta información resulta especialmente útil para modelar la continuidad temporal de los objetos en movimiento.
      </p>

      <p>
      En este trabajo se utiliza el método de Lucas–Kanade, un algoritmo diferencial que asume desplazamientos pequeños entre fotogramas y calcula vectores de movimiento
      a partir de puntos característicos previamente detectados. En particular, el flujo óptico se aplica sobre puntos internos de cada caja delimitadora detectada,
      permitiendo estimar el desplazamiento del objeto completo entre fotogramas consecutivos.
      </p>

      <li><strong>Integración de detección y seguimiento (Tracking-by-Detection)</strong></li>
      <p>
      La integración de detección y seguimiento se enmarca dentro del paradigma <i>tracking-by-detection</i>, ampliamente utilizado en sistemas modernos de seguimiento
      multiobjeto <a href="#sun2021">(Sun et al., 2021)</a>. En este enfoque, la detección de objetos se realiza de forma independiente en cada fotograma, mientras que el
      seguimiento se encarga de asociar dichas detecciones a lo largo del tiempo.
      </p>

      <p>
      En el pipeline desarrollado, YOLOv8 detecta peatones en cada fotograma y genera las cajas iniciales. Posteriormente, el módulo de seguimiento emplea flujo óptico
      para predecir la posición de los objetos en el siguiente fotograma y utiliza un criterio de emparejamiento basado en la intersección sobre unión (IoU) para asociar
      las detecciones actuales con los tracks existentes. A cada objeto se le asigna un identificador único que se mantiene mientras el seguimiento sea consistente.
      </p>

      <p>
      Los tracks que no logran asociarse durante varios fotogramas consecutivos son eliminados, mientras que nuevas detecciones generan nuevos identificadores.
      Esta estrategia permite combinar la precisión espacial de los modelos de detección profunda con la coherencia temporal proporcionada por el flujo óptico,
      dando como resultado un sistema eficiente y robusto para el seguimiento de peatones en secuencias de video.
      </p>

    </ul>

      
    <h2 id="metodologia">Metodología</h2>

    <p>
    La metodología desarrollada se basa en un enfoque <i>tracking-by-detection</i> que integra un detector YOLOv8 con un módulo de seguimiento mediante flujo óptico.
    El objetivo principal es detectar peatones en cada fotograma y mantener su identidad a lo largo del tiempo dentro de una secuencia de imágenes.
    Para este fin se empleó el dataset MOT17, el cual contiene secuencias urbanas anotadas en formato COCO-like.
    </p>

    <p>
    Las imágenes y anotaciones fueron cargadas y normalizadas mediante herramientas propias del proyecto, implementadas en el módulo
    <code>data_loader.py</code>, el cual se encarga de unificar rutas, estructuras y formatos de anotación. Esto permitió construir un pipeline
    reproducible y compatible con distintos conjuntos de imágenes y secuencias.
    </p>

    <p>
    La etapa de detección se ejecutó utilizando YOLOv8 con pesos preentrenados proporcionados por la librería Ultralytics.
    Cada fotograma fue procesado a través del wrapper definido en <code>detector.py</code>, generando como salida un conjunto de cajas delimitadoras
    en formato XYXY, junto con sus respectivas clases y puntajes de confianza. Estas detecciones constituyen la base para el proceso de seguimiento.
    </p>

    <p>
    El seguimiento de objetos se implementó mediante un tracker basado en flujo óptico piramidal de Lucas–Kanade, definido en
    <code>tracker.py</code>. Para cada detección activa se seleccionaron puntos característicos dentro de la caja delimitadora utilizando el método
    de Shi–Tomasi. A partir de estos puntos, se estimó el desplazamiento entre fotogramas consecutivos, lo que permitió predecir la nueva
    posición de cada objeto.
    </p>

    <p>
    Las predicciones obtenidas mediante flujo óptico se asociaron con las detecciones del fotograma actual utilizando el algoritmo de asignación
    Hungarian, empleando como métrica de similitud la intersección sobre unión (IoU). Este proceso garantizó una asignación consistente de
    identificadores (IDs) a lo largo del tiempo. El sistema creó nuevos tracks cuando aparecieron objetos no previamente observados y eliminó
    aquellos que permanecieron varios fotogramas sin correspondencias válidas.
    </p>

    <p>
    Todo el flujo fue integrado en el script <code>run_pipeline.py</code>, encargado de coordinar la detección, el seguimiento, el registro de resultados
    y la evaluación del sistema. Como salida, el pipeline generó archivos CSV con las detecciones y trayectorias, métricas cuantitativas de
    desempeño (precisión, recall y F1-score), así como un video anotado que visualiza las cajas delimitadoras, los identificadores
    de seguimiento y la evolución temporal de los objetos detectados.
    </p>

    
    <h2 id="resultados">Resultados y Análisis</h2>

    <p>
    El sistema de detección y seguimiento fue evaluado sobre secuencias del dataset MOT17,
    procesando un subconjunto representativo de fotogramas mediante el pipeline desarrollado.
    Los resultados se analizan tanto desde una perspectiva exploratoria del dataset
    como desde el desempeño del sistema de detección y seguimiento.
    </p>

    <h3>Análisis exploratorio del dataset</h3>

    <p>
    En primer lugar, se realizó un análisis exploratorio de las anotaciones del dataset con el fin
    de caracterizar la complejidad de las escenas y justificar los desafíos del problema.
    La <strong>Figura&nbsp;1</strong> muestra la distribución del número de peatones por imagen,
    evidenciando una alta variabilidad entre fotogramas, con escenas poco congestionadas
    y otras con alta densidad de personas.
    </p>

    <div class="figure">
      <img 
        src="results/exploratorio/mot17/objects_per_image_hist.png"/>
      <div class="caption">
        <strong>Figura 1.</strong> Distribución del número de peatones por imagen en el dataset MOT17.
      </div>
    </div>

    <p>
    La <strong>Figura&nbsp;2</strong> presenta la relación entre el ancho y el alto de las bounding boxes.
    Se observa una correlación positiva que refleja la diversidad de escalas de los peatones
    en la escena, producto de la perspectiva de la cámara y la distancia relativa de los objetos.
    </p>

    <div class="figure">
      <img 
        src="results/exploratorio/mot17/bbox_width_vs_height.png"/>
      <div class="caption">
        <strong>Figura 2.</strong> Relación ancho–alto de las cajas delimitadoras.
      </div>
    </div>

    <p>
    Por su parte, la <strong>Figura&nbsp;3</strong> ilustra la densidad de peatones por fotograma a lo largo
    de la secuencia, mostrando cambios abruptos asociados a transiciones de escena y
    zonas con mayor o menor concentración de objetos. Finalmente, la <strong>Figura&nbsp;4</strong>
    muestra la distribución de la duración de los tracks, donde se aprecia que la mayoría
    de los peatones son observados durante pocos fotogramas, mientras que un número
    reducido mantiene trayectorias largas y estables.
    </p>

    <div class="figure">
      <img src="results/exploratorio/mot17/objects_per_frame.png"/>
      <div class="caption">
        <strong>Figura 3.</strong> Número de peatones por fotograma en la secuencia MOT17.
      </div>
    </div>

    <div class="figure">
      <img src="results/exploratorio/mot17/track_durations.png"/>
      <div class="caption">
        <strong>Figura 4.</strong> Distribución de la duración de los tracks en número de frames.
      </div>
    </div>

    <h3>Resultados de detección y seguimiento</h3>

    <p>
    En la etapa de detección, YOLOv8 generó predicciones consistentes en la mayoría de los
    fotogramas procesados, permitiendo inicializar correctamente los tracks y alimentar
    el módulo de seguimiento basado en flujo óptico. La <strong>Figura&nbsp;5</strong> muestra ejemplos
    representativos de detección y seguimiento sobre distintas escenas,
    donde se observa un buen ajuste espacial de las cajas delimitadoras incluso en presencia
    de oclusiones parciales.
    </p>

    <div class="figure">
      <img src="results/cis_val_run/figures/sample_frame_2.png"/>
      <div class="caption">
        <strong>Figura 5.</strong>. Ejemplo representativo de detección y seguimiento con IDs persistentes.
      </div>
    </div>

    <p>
    El módulo de seguimiento logró mantener identidades coherentes para la mayoría de los peatones visibles.
    Los identificadores asignados permanecen estables a lo largo de varios fotogramas,
    incluso ante pequeños desplazamientos y cambios de escala. La asociación detección–track
    se realizó mediante una métrica IoU y el algoritmo Hungarian, lo que redujo la creación
    innecesaria de nuevos IDs.
    </p>

    <p>
    Desde el punto de vista cuantitativo, se calcularon métricas de detección utilizando un umbral
    IoU de 0.5. La <strong>Figura&nbsp;6</strong> muestra los valores de precisión, recall y F1-score obtenidos.
    El sistema presenta una <strong>alta precisión</strong>, lo que indica un bajo número de falsos positivos,
    mientras que el recall es más bajo, reflejando pérdidas de detección en escenas con alta
    congestión u oclusiones severas.
    </p>

    <div class="figure">
      <img src="results/cis_val_run/figures/detection_metrics.png"/>
      <div class="caption">
        <strong>Figura 6.</strong> Métricas de detección (IoU = 0.5).
      </div>
    </div>

    <p>
    Adicionalmente, la <strong>Figura&nbsp;7</strong> muestra la evolución del número de detecciones por fotograma,
    evidenciando un comportamiento estable y coherente con la dinámica de la escena.
    </p>

    <div class="figure">
      <img src="results/cis_val_run/figures/detections_per_frame.png"/>
      <div class="caption">
        <strong>Figura 7.</strong> Evolución del número de detecciones por fotograma.
      </div>
    </div>

    <h3>Evaluación cualitativa en video</h3>

    <p>
    Como resultado final, se generó un video anotado que permite evaluar visualmente el
    comportamiento del sistema en una secuencia continua. En este video se observan las
    cajas de detección, los identificadores asignados y la evolución temporal de cada peatón,
    lo que permite validar cualitativamente la coherencia del seguimiento.
    </p>

    <div class="figure">
      <video 
        controls 
        preload="auto" 
        muted
        playsinline>
        <source src="results/cis_val_run/tracking_result_web.mp4" type="video/mp4">
        Tu navegador no soporta la etiqueta video.
      </video>
      <div class="caption">
        <strong>Video 1.</strong> Secuencia MOT17 con detección y seguimiento de peatones.
      </div>
    </div>

    <h3>Discusión</h3>

    <p>
    En conjunto, los resultados muestran que la estrategia <em>tracking-by-detection</em>
    implementada es efectiva para escenas urbanas con múltiples peatones.
    La combinación de detección profunda con técnicas clásicas de flujo óptico logra
    un balance adecuado entre precisión espacial y coherencia temporal.
    </p>

    <p>
    Las principales limitaciones se presentan en escenarios con oclusiones prolongadas
    y alta densidad de objetos, donde algunos tracks se fragmentan o se pierden temporalmente.
    A pesar de ello, el sistema demuestra un desempeño sólido y constituye una base funcional
    para extensiones futuras, como el uso de métricas MOT avanzadas (MOTA, MOTP)
    o la incorporación de modelos de seguimiento basados en aprendizaje profundo.
    </p>


    <h2 id="conclusion">Conclusión</h2>
    <p>
    El sistema desarrollado demuestra que la combinación de un detector profundo como YOLOv8 con un módulo de seguimiento basado en flujo óptico constituye una 
    solución eficiente para el seguimiento de objetos en secuencias de video. La integración bajo un enfoque <em>tracking-by-detection</em> permitió aprovechar la 
    precisión espacial del modelo de detección y la coherencia temporal del flujo óptico, logrando trayectorias estables y manteniendo la identidad de los objetos 
    incluso ante desplazamientos moderados y variaciones en la escena.
    </p>
    <p>
    Los experimentos realizados evidenciaron que el pipeline es capaz de procesar secuencias completas, generar métricas cuantitativas confiables y producir videos 
    anotados que facilitan la validación visual del desempeño. Si bien el sistema presenta limitaciones frente a oclusiones prolongadas, alta densidad de objetos o 
    movimientos abruptos, en términos generales mostró un comportamiento robusto y consistente. En conjunto, los resultados indican que esta arquitectura es adecuada 
    para aplicaciones de monitoreo automatizado y constituye una base sólida para trabajos futuros, tales como la incorporación de métricas MOT avanzadas (MOTA, MOTP), 
    métodos de predicción más sofisticados o el entrenamiento de modelos especializados para escenarios específicos.
    </p>

    <h2 id="reporte">Reporte de contribución individual</h2>
    <p>El desarrollo de este proyecto se realizó de manera colectiva, manteniendo una comunicación constante entre los integrantes del equipo. 
      Las tareas fueron distribuidas de siguiente manera:</p>
    <ul>
      <li>Reinaldo David Lopez Narvaez: Desarrollo e implementación de los módulos en Python, incluyendo detección con YOLOv8, seguimiento por flujo óptico e integración del pipeline. </li>
      <li>Santiago Betancur Montoya: Análisis exploratorio de los datos y pruebas iniciales mediante el notebook exploratorio.</li>
      <li>Monica Paola Vargas Tirado: Redacción de la documentación del proyecto (introducción, marco teórico y metodología).</li>
      <li>Jose Sebastian Garzon Parra: Redacción de resultados, conclusiones y referencias. </li>
    </ul>
     

    <h2 id="referencias">Referencias bibliográficas</h2>
    <ol class="references">
    
      <li id="gongora2025">
        Góngora Tun, A. E. (2025).
        <em>Desarrollo de un algoritmo evolutivo con una codificación simbólica para estimar el desplazamiento de objetos</em>.
      </li>

      <li id="monzon2013">
        Monzón López, N. M. (2013).
        <em>Estimación del movimiento de objetos en secuencias de imágenes de intensidad y profundidad</em>.
      </li>

      <li id="lizardi2025">
        Lizardi Vargas, J. P. (2025).
        <em>Implementación y evaluación de algoritmos de seguimiento para el procesamiento de video</em>.
      </li>

      <li id="sun2021">
        Sun, Z., Chen, J., Chao, L., Ruan, W., & Mukherjee, M. (2021).
        <em>A Survey of Multiple Pedestrian Tracking Based on Tracking-by-Detection Framework. IEEE Transactions on Circuits and Systems for Video Technology, 31(5), 1819–1833.</em>
        <a href="https://doi.org/10.1109/TCSVT.2020.3009717" target="_blank"> https://doi.org/10.1109/TCSVT.2020.3009717</a>
      </li>

      <li id="valencia2025">
        Valencia Henao, J. M. (2025).
        <em>Clasificación automatizada de especies de vida silvestre en áreas protegidas a partir de imágenes de cámaras trampa</em>.
      </li>
    </ul>
  </main>

  <script>
    lucide.createIcons();
    const menuBtn = document.getElementById("menuBtn");
    const sidebar = document.getElementById("sidebar");
    const main = document.getElementById("main");

    // Estado inicial (sidebar visible en desktop)
    let isCollapsed = false;

    const toggleMenu = () => {
      // En pantallas pequeñas
      if (window.innerWidth <= 900) {
        sidebar.classList.toggle("active");
      } else {
        isCollapsed = !isCollapsed;
        sidebar.classList.toggle("collapsed", isCollapsed);
        main.classList.toggle("collapsed", isCollapsed);
      }
    };

    menuBtn.addEventListener("click", toggleMenu);

    // Cerrar el menú si se hace clic fuera en móvil
    document.addEventListener("click", (e) => {
      if (window.innerWidth <= 900 && !sidebar.contains(e.target) && !menuBtn.contains(e.target)) {
        sidebar.classList.remove("active");
      }
    });
  </script>
</body>
</html>


